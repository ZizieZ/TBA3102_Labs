{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c885e929-df38-413e-8361-bcb30ca331f8",
   "metadata": {},
   "source": [
    "# TBA 3102 - Text Analytics\n",
    "## Practical Lab 07 - Text Classification\n",
    "### Question 3 - Intermediate Text Classification\n",
    "Student: Nicky Ng <br>\n",
    "GitHub User: [ahjimomo](https://github.com/ahjimomo) <br>\n",
    "Student Number: A0194330L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5e5691-7524-4d4b-a507-d309d26c46ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "# Data Wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Data Preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Bow\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Tf-IDf\n",
    "from gensim.models import word2vec                          # Embeddings\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Cross-Validation & Model Evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import model_evaluation_utils as meu\n",
    "from sklearn import metrics\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Random_state\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1097f430-e013-4966-b5a3-c0ca0305ab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5559 entries, 0 to 5558\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Label            5559 non-null   object\n",
      " 1   SMSText          5559 non-null   object\n",
      " 2   Cleaned_SMSText  5559 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 130.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "raw_df = pd.read_csv('./data/sms_cleaned.csv')\n",
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbcec55b-cc1e-44a3-9b01-6eb8ac49e389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create label number\n",
    "raw_df['Label_no'] = np.where(raw_df['Label'] == 'ham', 1, 0)\n",
    "raw_df['Label_no'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db68e0d6-af4e-45fc-8fa2-8eecacccb48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training corpus: 4447\n",
      "Size of testing corpus: 1112\n",
      "Total: 5559\n"
     ]
    }
   ],
   "source": [
    "# Split training & testing data\n",
    "x_train_corpus, x_test_corpus, y_train_label_nums, y_test_label_nums, y_train_label_names, y_test_label_names = train_test_split(np.array(raw_df['Cleaned_SMSText']),\n",
    "                                                                                                                        np.array(raw_df['Label_no']),\n",
    "                                                                                                                        np.array(raw_df['Label']),\n",
    "                                                                                                                        test_size = 0.20, \n",
    "                                                                                                                        stratify = raw_df['Label_no'],\n",
    "                                                                                                                        shuffle = True,\n",
    "                                                                                                                        random_state = random_state)\n",
    "\n",
    "print(f\"Size of training corpus: {len(x_train_corpus)}\\nSize of testing corpus: {len(x_test_corpus)}\\nTotal: {len(raw_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d907e9a-9c46-4f18-b950-a2a0a35fcf31",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee53b7f-6e28-4a80-b808-7d9257a656b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency BOW model:> Train features shape: (4447, 4516)  Test features shape: (1112, 4516)\n"
     ]
    }
   ],
   "source": [
    "# 1. Term Frequency Features\n",
    "tf_cv = CountVectorizer(binary = False, min_df = 0.0, max_df = 1.0) \n",
    "\n",
    "# transform train & test data into features\n",
    "tf_train_features = tf_cv.fit_transform(x_train_corpus)\n",
    "tf_test_features = tf_cv.transform(x_test_corpus)\n",
    "\n",
    "print('Term Frequency BOW model:> Train features shape:', tf_train_features.shape, ' Test features shape:', tf_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf403d7c-b187-47cd-b8c2-98905d2d0228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Gram BOW model:> Train features shape: (4447, 4516)  Test features shape: (1112, 4516)\n"
     ]
    }
   ],
   "source": [
    "# 2. Unigram & Bigram Features\n",
    "ngram_cv = CountVectorizer(ngram_range=(2,2), min_df = 0.0, max_df = 1.0) \n",
    "\n",
    "# transform train & test data into features\n",
    "ngram_train_features = ngram_cv.fit_transform(x_train_corpus)\n",
    "ngram_test_features = ngram_cv.transform(x_test_corpus)\n",
    "\n",
    "print('N-Gram BOW model:> Train features shape:', tf_train_features.shape, ' Test features shape:', tf_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a8a29f-87c2-4806-81a8-2efdd4128350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF model:> Train features shape: (4447, 4516)  Test features shape: (1112, 4516)\n"
     ]
    }
   ],
   "source": [
    "# 3. Tf-IDf Features\n",
    "tfidf_tv = TfidfVectorizer(use_idf = True, min_df = 0.0, max_df = 1.0)\n",
    "\n",
    "# transform train & test data into features\n",
    "tv_train_features = tfidf_tv.fit_transform(x_train_corpus)\n",
    "tv_test_features = tfidf_tv.transform(x_test_corpus)\n",
    "\n",
    "print('TF-IDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc95cc-1015-46b6-b4d4-3f8cea5a174a",
   "metadata": {},
   "source": [
    "## Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc71e303-9145-4e5a-b558-94dd8f6297f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Comparison DataFrame containing all results\n",
    "model_names = ['mnb_tf', 'mnb_ngram', 'mnb_tfidf', 'lr_tf', 'lr_ngram', 'lr_tfidf', 'svm_tf', 'svm_ngram', 'svm_tfidf']\n",
    "final_df = pd.DataFrame(index = model_names)\n",
    "final_df\n",
    "\n",
    "# Preparation\n",
    "vectors_lst = ['tf', 'ngram', 'tfidf']\n",
    "train_features_lst = [tf_train_features, ngram_train_features, tv_train_features]\n",
    "test_features_lst = [tf_test_features, ngram_test_features, tv_test_features]\n",
    "unique_classes = list(set(y_test_label_names))\n",
    "\n",
    "# List to store results\n",
    "training_acc = []\n",
    "testing_acc = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "spam_precision = []\n",
    "spam_recall = []\n",
    "spam_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf16fe6-f616-4843-aa4e-c7c53f0902a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to iterate through models to build models\n",
    "def faster_models(models, algo, vector_type, train_features, test_features, train_labels = y_train_label_names, test_labels = y_test_label_names):\n",
    "   \n",
    "    # Define string param\n",
    "    for idx in range(len(vector_type)):\n",
    "        model = models[idx]\n",
    "        training_features = train_features[idx]\n",
    "        testing_features = test_features[idx]\n",
    "        predict_name = f\"{algo}_{vector_type[idx]}_predictions\"\n",
    "        print(predict_name)\n",
    "        \n",
    "        # Fit model & get mean training score\n",
    "        model.fit(training_features, train_labels)\n",
    "        training_acc.append(np.round(np.mean(cross_val_score(model, training_features, train_labels, cv = 5)), decimals = 2))\n",
    "        \n",
    "        # Use model to predict testing data\n",
    "        predict_name = model.predict(testing_features)\n",
    "        \n",
    "        # Extract performances\n",
    "        # Model\n",
    "        testing_acc.append(np.round(metrics.accuracy_score(test_labels, predict_name), decimals = 2))\n",
    "        precision.append(np.round(metrics.precision_score(test_labels, predict_name, average = 'weighted'), decimals = 2))\n",
    "        recall.append(np.round(metrics.recall_score(test_labels, predict_name, average = 'weighted'), decimals = 2))\n",
    "        f1.append(np.round(metrics.f1_score(test_labels, predict_name, average = 'weighted'), decimals = 2))\n",
    "        \n",
    "        # Spam\n",
    "        report = metrics.classification_report(y_true = test_labels, y_pred = predict_name, labels = unique_classes, output_dict = True)\n",
    "        spam_results = list(report['spam'].values())\n",
    "        spam_precision.append(np.round(spam_results[0], 2))\n",
    "        spam_recall.append(np.round(spam_results[1], 2))\n",
    "        spam_f1.append(np.round(spam_results[2], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e68b338-bc95-47b0-a2ea-f5174e6c3ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_tf_predictions\n",
      "mnb_ngram_predictions\n",
      "mnb_tfidf_predictions\n",
      "[0.97, 0.82, 0.95]\n",
      "[0.98, 0.97, 0.96]\n",
      "[0.98, 0.97, 0.96]\n",
      "[0.98, 0.97, 0.96]\n",
      "[0.98, 0.97, 0.96]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "mnb_tf = MultinomialNB(alpha = 1)\n",
    "mnb_ngram = MultinomialNB(alpha = 1)\n",
    "mnb_tfidf = MultinomialNB(alpha = 1)\n",
    "\n",
    "models = [mnb_tf, mnb_ngram, mnb_tfidf]\n",
    "\n",
    "faster_models(models, 'mnb', vectors_lst, train_features_lst, test_features_lst)\n",
    "print(training_acc)\n",
    "print(testing_acc)\n",
    "print(recall)\n",
    "print(precision)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f2493e2-cb4b-4924-8fe2-8d012d53119d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_tf_predictions\n",
      "lr_ngram_predictions\n",
      "lr_tfidf_predictions\n",
      "[0.97, 0.82, 0.95, 0.98, 0.94, 0.95]\n",
      "[0.98, 0.97, 0.96, 0.97, 0.95, 0.96]\n",
      "[0.98, 0.97, 0.96, 0.97, 0.95, 0.96]\n",
      "[0.98, 0.97, 0.96, 0.97, 0.95, 0.96]\n",
      "[0.98, 0.97, 0.96, 0.97, 0.95, 0.96]\n"
     ]
    }
   ],
   "source": [
    "# Logistics Regression\n",
    "lr_tf = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=random_state, solver='lbfgs')\n",
    "lr_ngram = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=random_state, solver='lbfgs')\n",
    "lr_tfidf = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=random_state, solver='lbfgs')\n",
    "\n",
    "models = [lr_tf, lr_ngram, lr_tfidf]\n",
    "\n",
    "faster_models(models, 'lr', vectors_lst, train_features_lst, test_features_lst)\n",
    "print(training_acc)\n",
    "print(testing_acc)\n",
    "print(recall)\n",
    "print(precision)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdff4c3b-9077-4fe6-98b4-d6cb178fb97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_tf_predictions\n",
      "lr_ngram_predictions\n",
      "lr_tfidf_predictions\n",
      "[0.97, 0.82, 0.95, 0.98, 0.94, 0.95, 0.98, 0.95, 0.98]\n",
      "[0.98, 0.97, 0.96, 0.97, 0.95, 0.96, 0.98, 0.96, 0.98]\n",
      "[0.98, 0.97, 0.96, 0.97, 0.95, 0.96, 0.98, 0.96, 0.98]\n",
      "[0.98, 0.97, 0.96, 0.97, 0.95, 0.96, 0.98, 0.96, 0.98]\n",
      "[0.98, 0.97, 0.96, 0.97, 0.95, 0.96, 0.98, 0.96, 0.98]\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine (SVM)\n",
    "svm_tf = LinearSVC(penalty='l2', max_iter=10000, C=1, random_state=random_state)\n",
    "svm_ngram = LinearSVC(penalty='l2', max_iter=10000, C=1, random_state=random_state)\n",
    "svm_tfidf = LinearSVC(penalty='l2', max_iter=10000, C=1, random_state=random_state)\n",
    "\n",
    "models = [svm_tf, svm_ngram, svm_tfidf]\n",
    "\n",
    "faster_models(models, 'lr', vectors_lst, train_features_lst, test_features_lst)\n",
    "print(training_acc)\n",
    "print(testing_acc)\n",
    "print(recall)\n",
    "print(precision)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bce4afe-0e39-4bf0-bf43-6318c43a43b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_training_acc</th>\n",
       "      <th>testing_acc</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>spam_recall</th>\n",
       "      <th>spam_precision</th>\n",
       "      <th>spam_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mnb_tf</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnb_ngram</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnb_tfidf</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_tf</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_ngram</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_tfidf</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm_tf</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm_ngram</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm_tfidf</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean_training_acc  testing_acc  recall  precision    f1  \\\n",
       "mnb_tf                  0.97         0.98    0.98       0.98  0.98   \n",
       "mnb_ngram               0.82         0.97    0.97       0.97  0.97   \n",
       "mnb_tfidf               0.95         0.96    0.96       0.96  0.96   \n",
       "lr_tf                   0.98         0.97    0.97       0.97  0.97   \n",
       "lr_ngram                0.94         0.95    0.95       0.95  0.95   \n",
       "lr_tfidf                0.95         0.96    0.96       0.96  0.96   \n",
       "svm_tf                  0.98         0.98    0.98       0.98  0.98   \n",
       "svm_ngram               0.95         0.96    0.96       0.96  0.96   \n",
       "svm_tfidf               0.98         0.98    0.98       0.98  0.98   \n",
       "\n",
       "           spam_recall  spam_precision  spam_f1  \n",
       "mnb_tf            0.85            0.97     0.91  \n",
       "mnb_ngram         0.81            1.00     0.89  \n",
       "mnb_tfidf         0.69            1.00     0.82  \n",
       "lr_tf             0.83            0.98     0.89  \n",
       "lr_ngram          0.64            1.00     0.78  \n",
       "lr_tfidf          0.73            0.98     0.84  \n",
       "svm_tf            0.86            0.96     0.90  \n",
       "svm_ngram         0.70            1.00     0.82  \n",
       "svm_tfidf         0.85            0.99     0.92  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile result df\n",
    "final_df['mean_training_acc'] = training_acc\n",
    "final_df['testing_acc'] = testing_acc\n",
    "final_df['recall'] = recall\n",
    "final_df['precision'] = precision\n",
    "final_df['f1'] = f1\n",
    "final_df['spam_recall'] = spam_recall\n",
    "final_df['spam_precision'] = spam_precision\n",
    "final_df['spam_f1'] = spam_f1\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d83c7ef-fc45-4f6a-b639-501d4b526132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as csv\n",
    "final_df.to_csv('./data/lab_7_models_comparison.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
